{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f8e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import errno\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import io\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pdb\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import pdb\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75d470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(var):\n",
    "    \"\"\"Exports torch.Tensor to Numpy array.\n",
    "    \"\"\"\n",
    "    return var.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def create_folder(folder_path):\n",
    "    \"\"\"Create a folder if it does not exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(folder_path)\n",
    "    except OSError as _e:\n",
    "        if _e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "\n",
    "def clear_folder(folder_path):\n",
    "    \"\"\"Clear all contents recursively if the folder exists.\n",
    "    Create the folder if it has been accidently deleted.\n",
    "    \"\"\"\n",
    "    create_folder(folder_path)\n",
    "    for the_file in os.listdir(folder_path):\n",
    "        _file_path = os.path.join(folder_path, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(_file_path):\n",
    "                os.unlink(_file_path)\n",
    "            elif os.path.isdir(_file_path):\n",
    "                shutil.rmtree(_file_path)\n",
    "        except OSError as _e:\n",
    "            print(_e)\n",
    "\n",
    "\n",
    "class StdOut(object):\n",
    "    \"\"\"Redirect stdout to file, and print to console as well.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_file):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(output_file, \"a\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.terminal.flush()\n",
    "        self.log.write(message)\n",
    "        self.log.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.log.flush()\n",
    "\n",
    "\n",
    "def boolean_string(s):\n",
    "    if s not in {'False', 'True'}:\n",
    "        raise ValueError('Not a valid boolean string')\n",
    "    return s == 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef53e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a custom dataset class which exports the HDF5 \n",
    "# Original dataset into a pytorch utils.data.Dataset here \n",
    "\n",
    "class Text2ImageDataset(Dataset):        # A subclass of the Pytorch dataset here \n",
    "\n",
    "    def __init__(self, datasetFile, transform=None, split=0):\n",
    "        self.datasetFile = datasetFile\n",
    "        self.transform = transform\n",
    "        self.dataset = None\n",
    "        self.dataset_keys = None\n",
    "        self.split = 'train' if split == 0 else 'valid' if split == 1 else 'test'\n",
    "        self.h5py2int = lambda x: int(np.array(x))\n",
    "\n",
    "    def __len__(self):\n",
    "        f = h5py.File(self.datasetFile, 'r')\n",
    "        self.dataset_keys = [str(k) for k in f[self.split].keys()]\n",
    "        length = len(f[self.split])\n",
    "        f.close()\n",
    "\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.dataset is None:\n",
    "            self.dataset = h5py.File(self.datasetFile, mode='r')\n",
    "            self.dataset_keys = [str(k) for k in self.dataset[self.split].keys()]\n",
    "\n",
    "        example_name = self.dataset_keys[idx]\n",
    "        example = self.dataset[self.split][example_name]\n",
    "\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        right_image = bytes(np.array(example['img']))\n",
    "        right_embed = np.array(example['embeddings'], dtype=float)\n",
    "        wrong_image = bytes(np.array(self.find_wrong_image(example['class'])))\n",
    "        inter_embed = np.array(self.find_inter_embed())\n",
    "\n",
    "        right_image = Image.open(io.BytesIO(right_image)).resize((64, 64))\n",
    "        wrong_image = Image.open(io.BytesIO(wrong_image)).resize((64, 64))\n",
    "\n",
    "        right_image = self.validate_image(right_image)\n",
    "        wrong_image = self.validate_image(wrong_image)\n",
    "\n",
    "        txt = np.array(example['txt']).astype(str)\n",
    "\n",
    "        sample = {\n",
    "                'right_images': torch.FloatTensor(right_image),\n",
    "                'right_embed': torch.FloatTensor(right_embed),\n",
    "                'wrong_images': torch.FloatTensor(wrong_image),\n",
    "                'inter_embed': torch.FloatTensor(inter_embed),\n",
    "                'txt': str(txt)\n",
    "                 }\n",
    "\n",
    "        sample['right_images'] = sample['right_images'].sub_(127.5).div_(127.5)\n",
    "        sample['wrong_images'] =sample['wrong_images'].sub_(127.5).div_(127.5)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def find_wrong_image(self, category):\n",
    "        idx = np.random.randint(len(self.dataset_keys))\n",
    "        example_name = self.dataset_keys[idx]\n",
    "        example = self.dataset[self.split][example_name]\n",
    "        _category = example['class']\n",
    "\n",
    "        if _category != category:\n",
    "            return example['img']\n",
    "\n",
    "        return self.find_wrong_image(category)\n",
    "\n",
    "    def find_inter_embed(self):\n",
    "        idx = np.random.randint(len(self.dataset_keys))\n",
    "        example_name = self.dataset_keys[idx]\n",
    "        example = self.dataset[self.split][example_name]\n",
    "        return example['embeddings']\n",
    "\n",
    "\n",
    "    def validate_image(self, img):\n",
    "        img = np.array(img, dtype=float)\n",
    "        if len(img.shape) < 3:\n",
    "            rgb = np.empty((64, 64, 3), dtype=np.float32)\n",
    "            rgb[:, :, 0] = img\n",
    "            rgb[:, :, 1] = img\n",
    "            rgb[:, :, 2] = img\n",
    "            img = rgb\n",
    "\n",
    "        return img.transpose(2, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2c1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels, latent_dim=100, embed_dim=1024, embed_out_dim=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.latent_dim = latent_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_out_dim = embed_out_dim\n",
    "\n",
    "        self.text_embedding = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, self.embed_out_dim),\n",
    "            nn.BatchNorm1d(self.embed_out_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        model = []\n",
    "        model += self._create_layer(self.latent_dim + self.embed_out_dim, 512, 4, stride=1, padding=0)\n",
    "        # The noise vector and the text encoded vectors are conatenated \n",
    "        model += self._create_layer(512, 256, 4, stride=2, padding=1)\n",
    "        model += self._create_layer(256, 128, 4, stride=2, padding=1)\n",
    "        model += self._create_layer(128, 64, 4, stride=2, padding=1)\n",
    "        model += self._create_layer(64, self.channels, 4, stride=2, padding=1, output=True)\n",
    "\n",
    "        self.model = nn.Sequential(*model)      # This does away with the requirement of us defining a forward path here \n",
    "        \n",
    "\n",
    "    # Transpose Convolution Layers are present here :: to increase the filter activation map dimension \n",
    "    # The input to the generator would be a concatenation of the noise vector and the actual text vector \n",
    "    \n",
    "    def _create_layer(self, size_in, size_out, kernel_size=4, stride=2, padding=1, output=False):\n",
    "        layers = [nn.ConvTranspose2d(size_in, size_out, kernel_size, stride=stride, padding=padding, bias=False)]\n",
    "        if output:\n",
    "            layers.append(nn.Tanh())\n",
    "        else:\n",
    "            layers += [nn.BatchNorm2d(size_out),\n",
    "                        nn.ReLU(True)]\n",
    "        return layers\n",
    "\n",
    "    def forward(self, noise, text):\n",
    "        text = self.text_embedding(text)\n",
    "        text = text.view(text.shape[0], text.shape[1], 1, 1)\n",
    "        z = torch.cat([text, noise], 1)\n",
    "        return self.model(z)\n",
    "\n",
    "# The text vector is replicated in order to feed to the Discriminator here : \n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.text_embedding = nn.Sequential(\n",
    "            nn.Linear(size_in, size_out),\n",
    "            nn.BatchNorm1d(size_out),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, text):\n",
    "        embed_out = self.text_embedding(text)\n",
    "        embed_out_resize = embed_out.repeat(4, 4, 1, 1).permute(2, 3, 0, 1)\n",
    "        out = torch.cat([x, embed_out_resize], 1)\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels, embed_dim=1024, embed_out_dim=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_out_dim = embed_out_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *self._create_layer(self.channels, 64, 4, 2, 1, normalize=False),\n",
    "            *self._create_layer(64, 128, 4, 2, 1),\n",
    "            *self._create_layer(128, 256, 4, 2, 1),\n",
    "            *self._create_layer(256, 512, 4, 2, 1)\n",
    "        )\n",
    "        self.text_embedding = Embedding(self.embed_dim, self.embed_out_dim)       # The Text is basically replicated here \n",
    "        # And then stacked with the feature maps here \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(512 + self.embed_out_dim, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _create_layer(self, size_in, size_out, kernel_size=4, stride=2, padding=1, normalize=True):\n",
    "        layers = [nn.Conv2d(size_in, size_out, kernel_size=kernel_size, stride=stride, padding=padding)]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(size_out))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x, text):\n",
    "        x_out = self.model(x)       # Get a convolutional feature map \n",
    "        # The input x is either the real image or the fake image here \n",
    "        out = self.text_embedding(x_out, text)     # Do stacking here \n",
    "        out = self.output(out)                     # Pass through a convolution again and if required a ANN to get the outputs \n",
    "        return out.squeeze(), x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "646ef0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self,\n",
    "                 name,\n",
    "                 device,\n",
    "                 data_loader,\n",
    "                 channels,\n",
    "                 l1_coef,\n",
    "                 l2_coef):\n",
    "        \n",
    "        self.name = name\n",
    "        self.device = device\n",
    "        self.data_loader = data_loader\n",
    "        self.channels = channels\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.netG = Generator(self.channels)\n",
    "        self.netG.apply(_weights_init)\n",
    "        self.netG.to(self.device)\n",
    "        self.netD = Discriminator(self.channels)\n",
    "        self.netD.apply(_weights_init)\n",
    "        self.netD.to(self.device)\n",
    "        self.optim_G = None\n",
    "        self.optim_D = None\n",
    "        self.loss_adv = torch.nn.BCELoss()\n",
    "        self.loss_l1 = torch.nn.L1Loss()\n",
    "        self.loss_l2 = torch.nn.MSELoss()\n",
    "\n",
    "    \n",
    "# The property() method in Python provides an interface to instance attributes. It encapsulates instance attributes and provides a property, same as Java and C#. \n",
    "# The property() method takes the get, set and delete methods as arguments and returns an object of the property class.    \n",
    "        \n",
    "    @property\n",
    "    def generator(self):\n",
    "        return self.netG\n",
    "\n",
    "    @property\n",
    "    def discriminator(self):\n",
    "        return self.netD\n",
    "\n",
    "    def create_optim(self, lr, alpha=0.5, beta=0.999):\n",
    "        self.optim_G = torch.optim.Adam(self.netG.parameters(),\n",
    "                                        lr=lr, betas=(alpha, beta))\n",
    "        \n",
    "        self.optim_D = torch.optim.Adam(self.netD.parameters(),\n",
    "                                        lr=lr, betas=(alpha, beta))\n",
    "\n",
    "    def train(self,\n",
    "              epochs,\n",
    "              log_interval=100,\n",
    "              out_dir='',\n",
    "              verbose=True):\n",
    "        \n",
    "        self.netG.train()\n",
    "        self.netD.train()          # In train mode \n",
    "        total_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            batch_time = time.time()\n",
    "            for batch_idx, data in enumerate(self.data_loader):\n",
    "                image = data['right_images'].to(self.device)       # Get only the correct images here \n",
    "                embed = data['right_embed'].to(self.device)\n",
    "\n",
    "                real_label = torch.ones((image.shape[0]), device=self.device)\n",
    "                fake_label = torch.zeros((image.shape[0]), device=self.device)\n",
    "\n",
    "                # Train D\n",
    "                self.optim_D.zero_grad()\n",
    "\n",
    "                out_real, _ = self.netD(image, embed)\n",
    "                loss_d_real = self.loss_adv(out_real, real_label)         # Adverserial Loss for the Disc.\n",
    "\n",
    "                noise = torch.randn((image.shape[0], 100, 1, 1), device=self.device)\n",
    "                image_fake = self.netG(noise, embed)\n",
    "                out_fake, _ = self.netD(image_fake, embed)\n",
    "                loss_d_fake = self.loss_adv(out_fake, fake_label)           # Adverserial Loss for the Disc.\n",
    "\n",
    "                d_loss = loss_d_real + loss_d_fake\n",
    "                d_loss.backward()\n",
    "                self.optim_D.step()                  # Train the Disc. \n",
    "\n",
    "                # Train G\n",
    "                self.optim_G.zero_grad()\n",
    "                noise = torch.randn((image.shape[0], 100, 1, 1), device=self.device)\n",
    "                image_fake = self.netG(noise, embed)\n",
    "                out_fake, act_fake = self.netD(image_fake, embed)   # Activation Map and the Fake Output here \n",
    "                _, act_real = self.netD(image, embed)               # \n",
    "\n",
    "                l1_loss = self.loss_l1(torch.mean(act_fake, 0), torch.mean(act_real, 0).detach())\n",
    "                # L1 Loss between the Activation Maps mean \n",
    "                \n",
    "                g_loss = self.loss_adv(out_fake, real_label) + \\\n",
    "                    self.l1_coef * l1_loss + \\\n",
    "                    self.l2_coef * self.loss_l2(image_fake, image) # Okay 1 adverserial loss term \n",
    "                    # One L1 loss term and one L2 loss term here \n",
    "                    \n",
    "                g_loss.backward()\n",
    "                self.optim_G.step()\n",
    "\n",
    "                if verbose and batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "                    print('Epoch {} [{}/{}] loss_D: {:.4f} loss_G: {:.4f} time: {:.2f}'.format(\n",
    "                          epoch, batch_idx, len(self.data_loader),\n",
    "                          d_loss.mean().item(),\n",
    "                          g_loss.mean().item(),\n",
    "                          time.time() - batch_time))\n",
    "                    with torch.no_grad():\n",
    "                        viz_sample = torch.cat((image[:32], image_fake[:32]), 0)\n",
    "                        vutils.save_image(viz_sample,\n",
    "                                          os.path.join(out_dir, 'samples_{}_{}.png'.format(epoch, batch_idx)),\n",
    "                                          nrow=8,\n",
    "                                          normalize=True)\n",
    "                    batch_time = time.time()\n",
    "\n",
    "            self.save_to(path=out_dir, name=self.name, verbose=False)\n",
    "        if verbose:\n",
    "            print('Total train time: {:.2f}'.format(time.time() - total_time))\n",
    "\n",
    "    def eval(self,\n",
    "             batch_size=None):\n",
    "        self.netG.eval()\n",
    "        self.netD.eval()\n",
    "        if batch_size is None:\n",
    "            batch_size = self.data_loader.batch_size\n",
    "\n",
    "        with torch.no_grad():                               # In evaluation mode here :: \n",
    "            for batch_idx, data in enumerate(self.data_loader):\n",
    "                image = data['right_images'].to(self.device)[:batch_size]\n",
    "                embed = data['right_embed'].to(self.device)[:batch_size]\n",
    "                text = data['txt'][:batch_size]\n",
    "                noise = torch.randn((image.shape[0], 100, 1, 1), device=self.device)\n",
    "                viz_sample = self.netG(noise, embed)\n",
    "                vutils.save_image(viz_sample,\n",
    "                                  'img_{}.png'.format(batch_idx),\n",
    "                                  nrow=batch_size//8,\n",
    "                                  normalize=True)\n",
    "                for t in text:\n",
    "                    print(t)\n",
    "                break\n",
    "\n",
    "    def save_to(self,\n",
    "                path='',\n",
    "                name=None,\n",
    "                verbose=True):\n",
    "        if name is None:\n",
    "            name = self.name\n",
    "        if verbose:\n",
    "            print('\\nSaving models to {}_G.pt and {}_D.pt ...'.format(name, name))\n",
    "        torch.save(self.netG, os.path.join(path, '{}_G.pt'.format(name)))\n",
    "        torch.save(self.netD, os.path.join(path, '{}_D.pt'.format(name)))\n",
    "\n",
    "    def load_from(self,\n",
    "                  path='',\n",
    "                  name=None,\n",
    "                  verbose=True):\n",
    "        if name is None:\n",
    "            name = self.name\n",
    "        if verbose:\n",
    "            print('\\nLoading models from {}_G.pt and {}_D.pt ...'.format(name, name))\n",
    "        ckpt_G = torch.load(os.path.join(path, '{}_G.pt'.format(name)))\n",
    "        if isinstance(ckpt_G, dict) and 'state_dict' in ckpt_G:\n",
    "            self.netG.load_state_dict(ckpt_G['state_dict'], strict=True)\n",
    "        elif isinstance(ckpt_G, torch.nn.Module):\n",
    "            self.netG = ckpt_G\n",
    "        else:\n",
    "            self.netG.load_state_dict(ckpt_G, strict=True)\n",
    "        ckpt_D = torch.load(os.path.join(path, '{}_D.pt'.format(name)))\n",
    "        if isinstance(ckpt_D, dict) and 'state_dict' in ckpt_D:\n",
    "            self.netD.load_state_dict(ckpt_D['state_dict'], strict=True)\n",
    "        elif isinstance(ckpt_D, torch.nn.Module):\n",
    "            self.netD = ckpt_D\n",
    "        else:\n",
    "            self.netD.load_state_dict(ckpt_D, strict=True)\n",
    "            \n",
    "            \n",
    "# A checkpoint is an intermediate dump of a model's entire internal state (its weights, current learning rate, etc.) \n",
    "# so that the framework can resume the training from this point whenever desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cde437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to D:\\Text-to-Image-GAN\\output\\log.txt\n",
      "\n",
      "PyTorch version: 1.9.0\n",
      "CUDA version: 10.2\n",
      "\n",
      "         Args         |    Type    |    Value\n",
      "--------------------------------------------------\n",
      "  model               |  str       |  text2image\n",
      "  cuda                |  bool      |  True\n",
      "  train               |  bool      |  True\n",
      "  data_dir            |  str       |  D:\\Text-to-Image-GAN\\\n",
      "  dataset             |  str       |  birds\n",
      "  out_dir             |  str       |  D:\\Text-to-Image-GAN\\output\\\n",
      "  epochs              |  int       |  200\n",
      "  batch_size          |  int       |  256\n",
      "  lr                  |  float     |  0.0002\n",
      "  channels            |  int       |  3\n",
      "  l1_coef             |  int       |  50\n",
      "  l2_coef             |  int       |  100\n",
      "  log_interval        |  int       |  20\n",
      "  seed                |  int       |  1\n",
      "Loading data...\n",
      "\n",
      "Creating model...\n",
      "\n",
      "Training...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "#     device = torch.device(\"cuda:0\" if FLAGS.cuda else \"cpu\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "    print('Loading data...\\n')\n",
    "    dataloader = DataLoader(Text2ImageDataset(os.path.join(FLAGS.data_dir, '{}.hdf5'.format(FLAGS.dataset)), split=0),\n",
    "                            batch_size=FLAGS.batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "    print('Creating model...\\n')\n",
    "    model = Model(FLAGS.model, device, dataloader, FLAGS.channels, FLAGS.l1_coef, FLAGS.l2_coef)\n",
    "\n",
    "    if FLAGS.train:\n",
    "        model.create_optim(FLAGS.lr)\n",
    "\n",
    "        print('Training...\\n')\n",
    "        model.train(FLAGS.epochs, FLAGS.log_interval, FLAGS.out_dir, True)\n",
    "\n",
    "        model.save_to('')\n",
    "    else:\n",
    "        model.load_from('')\n",
    "\n",
    "        print('Evaluating...\\n')\n",
    "        model.eval(batch_size=64)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='')\n",
    "    parser.add_argument('--model', type=str, default='text2image', help='text2image')\n",
    "    parser.add_argument('--cuda', default=True, help='enable CUDA.')\n",
    "    parser.add_argument('--train', default=True, help='train mode or eval mode.')\n",
    "    parser.add_argument('--data_dir', type=str, default='D:\\\\Text-to-Image-GAN\\\\', help='Directory for dataset.')\n",
    "    parser.add_argument('--dataset', type=str, default='birds', help='Dataset name.')\n",
    "    parser.add_argument('--out_dir', type=str, default='D:\\\\Text-to-Image-GAN\\\\output\\\\', help='Directory for output.')\n",
    "    parser.add_argument('--epochs', type=int, default=200, help='number of epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=256, help='size of batches in training')\n",
    "    parser.add_argument('--lr', type=float, default=0.0002, help='learning rate')\n",
    "    parser.add_argument('--channels', type=int, default=3, help='number of image channels')\n",
    "    parser.add_argument('--l1_coef', type=float, default=50, help='l1 coefficient')\n",
    "    parser.add_argument('--l2_coef', type=float, default=100, help='l2 coefficient')\n",
    "    parser.add_argument('--log_interval', type=int, default=20, help='interval between logging and image sampling')\n",
    "    parser.add_argument('--seed', type=int, default=1, help='random seed')\n",
    "\n",
    "    FLAGS, unknown = parser.parse_known_args()\n",
    "    FLAGS.cuda = FLAGS.cuda and torch.cuda.is_available()\n",
    "\n",
    "    if FLAGS.seed is not None:\n",
    "        torch.manual_seed(FLAGS.seed)\n",
    "        if FLAGS.cuda:\n",
    "            torch.cuda.manual_seed(FLAGS.seed)\n",
    "        np.random.seed(FLAGS.seed)\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    create_folder(FLAGS.out_dir)\n",
    "    if FLAGS.train:\n",
    "        clear_folder(FLAGS.out_dir)\n",
    "\n",
    "    log_file = os.path.join(FLAGS.out_dir, 'log.txt')\n",
    "    print(\"Logging to {}\\n\".format(log_file))\n",
    "    sys.stdout = StdOut(log_file)\n",
    "\n",
    "    print(\"PyTorch version: {}\".format(torch.__version__))\n",
    "    print(\"CUDA version: {}\\n\".format(torch.version.cuda))\n",
    "\n",
    "    print(\" \" * 9 + \"Args\" + \" \" * 9 + \"|    \" + \"Type\" + \\\n",
    "          \"    |    \" + \"Value\")\n",
    "    print(\"-\" * 50)\n",
    "    for arg in vars(FLAGS):\n",
    "        arg_str = str(arg)\n",
    "        var_str = str(getattr(FLAGS, arg))\n",
    "        type_str = str(type(getattr(FLAGS, arg)).__name__)\n",
    "        print(\"  \" + arg_str + \" \" * (20-len(arg_str)) + \"|\" + \\\n",
    "              \"  \" + type_str + \" \" * (10-len(type_str)) + \"|\" + \\\n",
    "              \"  \" + var_str)\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f7c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator_modified(nn.Module):\n",
    "    def __init__(self, channels, latent_dim=100, embed_dim=1024, embed_out_dim=128, text_stack = 10):\n",
    "        super(Generator, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.latent_dim = latent_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_out_dim = embed_out_dim\n",
    "        \n",
    "        self.text_stack = text_stack               # We are also stacking the text instead of just concatenating here \n",
    "\n",
    "        self.text_embedding = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, self.embed_out_dim),\n",
    "            nn.BatchNorm1d(self.embed_out_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        model = []\n",
    "        model += self._create_layer(self.latent_dim + self.embed_out_dim, 512, 4, stride=1, padding=0)\n",
    "        # The noise vector and the text encoded vectors are conatenated \n",
    "        model += self._create_layer(512, 256 + self.text_stack, 4, stride=2, padding=1)\n",
    "        model += self._create_layer(256, 128 + self.text_stack, 4, stride=2, padding=1)\n",
    "        model += self._create_layer(128, 64 + self.text_stack, 4, stride=2, padding=1)\n",
    "        model += self._create_layer(64 + self.text_stack, self.channels, 4, stride=2, padding=1, output=True)\n",
    "\n",
    "        self.model = nn.Sequential(*model)      # This does away with the requirement of us defining a forward path here \n",
    "        \n",
    "\n",
    "    # Transpose Convolution Layers are present here :: to increase the filter activation map dimension \n",
    "    # The input to the generator would be a concatenation of the noise vector and the actual text vector \n",
    "    \n",
    "    def _create_layer(self, size_in, size_out, kernel_size=4, stride=2, padding=1, output=False):\n",
    "        layers = [nn.ConvTranspose2d(size_in, size_out, kernel_size, stride=stride, padding=padding, bias=False)]\n",
    "        if output:\n",
    "            layers.append(nn.Tanh())\n",
    "        else:\n",
    "            layers += [nn.BatchNorm2d(size_out),\n",
    "                        nn.ReLU(True)]\n",
    "        return layers\n",
    "\n",
    "    def forward(self, noise, text):\n",
    "        text = self.text_embedding(text)\n",
    "        text = text.view(text.shape[0], text.shape[1], 1, 1)\n",
    "        z = torch.cat([text, noise], 1)\n",
    "        return self.model(z)\n",
    "\n",
    "# The text vector is replicated in order to feed to the Discriminator here : \n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.text_embedding = nn.Sequential(\n",
    "            nn.Linear(size_in, size_out),\n",
    "            nn.BatchNorm1d(size_out),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, text):\n",
    "        embed_out = self.text_embedding(text)\n",
    "        embed_out_resize = embed_out.repeat(4, 4, 1, 1).permute(2, 3, 0, 1)\n",
    "        out = torch.cat([x, embed_out_resize], 1)\n",
    "        return out\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
